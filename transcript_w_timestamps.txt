1
00:00:00,000 --> 00:00:08,560
As a freelance creative technologist, my early explorations into AI consisted of making weird art with models like StyleGAN, DeepDream, and StyleTransfer.

2
00:00:08,560 --> 00:00:17,200
But now, several years later into the evolution of AI, I've found myself using AI out of convenience and practicality for my coding, design, and writing work.

3
00:00:17,200 --> 00:00:29,360
The tasks for which I've enlisted AI include making logos and icons with stable diffusion, writing web copy with OpenAI, performing voice authentication with Microsoft's Cognitive Speech, and writing code with GitHub Copilot.

4
00:00:29,360 --> 00:00:38,960
This presentation, as you likely by now have noticed, is not actually a video of me, but a deepfake using voice generated from Descript's Overdub, and video using the software DeepFace.

5
00:00:38,960 --> 00:00:46,960
Keeping track of all of these emerging AI models is less important to me than the realization that machine learning has gone from niche use cases and like making puppy slugs,

6
00:00:46,960 --> 00:00:53,200
to being something that people, myself included, will use, almost unwittingly, to make their work faster and easier.

7
00:00:53,200 --> 00:00:58,720
Alongside my interest in creative AI, I have a keen interest in sustainability and the environment.

8
00:00:58,720 --> 00:01:03,440
As far as matches go, AI and the environment might seem to be a pair of star-crossed lovers.

9
00:01:03,440 --> 00:01:11,680
Training GPT-3, a model on which many of the services I listed above are based, required an estimated 85,000 kilograms of CO2.

10
00:01:11,680 --> 00:01:15,760
It would take 100 acres of forest an entire year to sequester that much carbon.

11
00:01:15,760 --> 00:01:19,440
Put differently, you could charge 10 million smartphones and have the same effect.

12
00:01:19,440 --> 00:01:27,920
The carbon footprint of AI is expected to grow at a continued rate of 44% per year through 2025, with computation costs doubling every few months.

13
00:01:27,920 --> 00:01:35,520
Someone asked a good question in the environment Slack channel about how much energy is expended in not training, but using, these AI-based services.

14
00:01:35,520 --> 00:01:49,360
I thought it was a good question and didn't know the answer either, so I did some digging and found this machine learning emissions calculator from some of the same folks who also put out a Python library called CodeCarbon, which can perform an estimation of your code's real-time carbon emissions.

15
00:01:49,360 --> 00:01:57,360
Running CodeCarbon I performed two tests, one using stable diffusion to generate images, and another doing a more typical hello world of machine learning tasks,

16
00:01:57,360 --> 00:02:01,040
training a model on the MNIST dataset, each for about five minutes.

17
00:02:01,040 --> 00:02:03,840
The output for both was about a third of a gram of carbon.

18
00:02:03,840 --> 00:02:09,600
In other words, I could use stable diffusion for about three hours and it would be the same as a full charge on my smartphone.

19
00:02:09,600 --> 00:02:17,520
I reached out to Sasha Luciani, one of the authors of CodeCarbon, and in Research Science of Ethical AI at Hugging Face, to check my thinking, and she said that,

20
00:02:17,520 --> 00:02:20,960
unlike model training, machine learning inference is tough to estimate.

21
00:02:20,960 --> 00:02:26,160
It depends on the size of the GPU, how many requests it is receiving, and where it is getting its energy.

22
00:02:26,160 --> 00:02:33,680
Using the aforementioned machine learning carbon calculator, I did some estimates using different cloud provider data centers, and learned that, for example,

23
00:02:33,680 --> 00:02:43,040
training for 100 hours in Google Cloud's Asia South region would output 23 kilograms of carbon, which is 46 times larger than if you were using the GCP Europe West region.

24
00:02:43,040 --> 00:02:50,880
This can be attributed to the fact that Europe West is using mostly renewable sources of energy, and Asia South has a grid driven primarily by fossil fuels.

25
00:02:50,960 --> 00:03:00,480
Additionally, as Sam Altman of OpenEye predicts, the future may likely be one in which we are by and large leveraging pre-trained AI models that are abstracted and tuned to specific use cases.

26
00:03:00,480 --> 00:03:07,040
However, it's important not to forget the downstream real-world benefits that AI models can deliver, many of which can be carbon reducing,

27
00:03:07,040 --> 00:03:13,600
such as traffic and energy forecasting and optimization, reducing system waste, and environmental monitoring and modeling.

28
00:03:14,560 --> 00:03:23,760
Not to mention the intangible benefits AI can bring, such as reducing the extreme social anxiety one can feel by giving a presentation live in front of an audience.

29
00:03:25,200 --> 00:03:30,560
On the future products team, we're looking for ways to leverage AI responsibly to achieve company goals,

30
00:03:30,560 --> 00:03:36,080
as well as minimize the environmental impact of technology both within the company and without.

31
00:03:36,080 --> 00:03:42,960
One particular open source contribution I'd love to work on with other mazilians is a JavaScript library that can interface with CodeCarbon.

32
00:03:43,040 --> 00:03:49,840
If you have knowledge, ideas, or curiosity in any of these topics, Future Products needs your expertise and would love to collaborate.

33
00:03:49,840 --> 00:03:51,520
Please get in touch. Thanks.
